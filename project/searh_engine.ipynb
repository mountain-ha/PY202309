{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 쿼리를 입력하세요.life is too short\n",
      "rank\tIndex\tscore\tsentence\n",
      "1\t190\t0.16666666666666666\tIt is Sunday.\n",
      "2\t314\t0.16666666666666666\tThis is Washington.\n",
      "3\t675\t0.16666666666666666\tSe-na: It's too hot today. When is your English test?\n",
      "4\t710\t0.16666666666666666\tTravel is exciting.\n",
      "5\t526\t0.14285714285714285\tBob is my brother.\n",
      "6\t538\t0.14285714285714285\tMy hobby is traveling.\n",
      "7\t679\t0.14285714285714285\tMy name is Mike.\n",
      "8\t45\t0.125\tThis method is called *acupuncture.\n",
      "9\t107\t0.125\tBut this is very interesting.\n",
      "10\t293\t0.125\tB : When is it?\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def preprocess(sentence):\n",
    "    preprocessed_sentence = sentence.strip().split(\" \") #입력받은 영어 쿼리를 공백을 기준으로 나눈 리스트로 만들기\n",
    "    return  preprocessed_sentence\n",
    "\n",
    "def indexing(file_name):\n",
    "    lines = open(file_name, \"r\", encoding=\"utf8\").readlines() #\"jhe-koen-dev.en\"파일 열기\n",
    "    file_tokens_pairs = [] #빈 리스트 \"file_tokens_pairs\" 만들기\n",
    "    for line in lines: \n",
    "        tokens = line.strip().split(\" \") #한 문장을 하나의 리스트로 하고 공백을 기준으로 문장을 나누어 원소로 만들기\n",
    "        file_tokens_pairs.append(tokens) #리스트 \"file_tokens_pairs\"에 바로 위에서 만들어진 리스트 추가하기\n",
    "    return file_tokens_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Indexing\n",
    "file_name = \"jhe-koen-dev.en\"\n",
    "file_tokens_pairs = indexing(file_name)\n",
    "\n",
    "# 2. Input the query\n",
    "query = input(\"영어 쿼리를 입력하세요.\") #영어 쿼리 입력받기\n",
    "preprocessed_query = preprocess(query) #입력받은 영어 쿼리를 공백을 기준으로 나눈 리스트로 만들기\n",
    "query_token_set = set(preprocessed_query) #바로 위에서 만든 리스트를 집합의 형태로 바꿔서 \"query_token_set\"에 저장하기\n",
    "\n",
    "# 3. Calculate similarities based on a same token set\n",
    "score_dict = {} #각 문장별로 점수를 메기고 기록하기위한 {인덱스:점수}딕셔너리 생성\n",
    "for i in range(len(file_tokens_pairs)):\n",
    "    file_token_set = set(file_tokens_pairs[i]) #file_tokens_pairs\"를 집합으로 만들기. 이 프로그램에서 사용된 유사도 구하는 법은 집합을 사용함\n",
    "    all_tokens = query_token_set | file_token_set #사용자가 입력한 문장과 파일 안에 있는 문장간의 합집합 원소 개수 \n",
    "    same_tokens = query_token_set & file_token_set #사용자가 입력한 문장과 파일 안에 있는 문장간의 교집합 원소 개수\n",
    "    similarity = len(same_tokens) / len(all_tokens) # 교집합 수/합집합 수 로 유사도 구하기\n",
    "    score_dict[i] = similarity #딕셔너리에 구한 유사도 추가\n",
    "    \n",
    "\n",
    "# 4. Sort the similarity list\n",
    "#유사도에 따라 내림차순으로 정렬한 리스트 생성\n",
    "sorted_score_list = sorted(score_dict.items(), key = operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# 5. Print the result\n",
    "if sorted_score_list[0][1] == 0.0: #가장 높은 유사도가 0이면 \"There is no similar sentence.\"출력\n",
    "    print(\"There is no similar sentence.\")\n",
    "else:\n",
    "    print(\"rank\", \"Index\", \"score\", \"sentence\", sep = \"\\t\")\n",
    "    rank = 1\n",
    "    for i, score  in sorted_score_list: #유사도가 높은 순서대로 10개 출력\n",
    "        print(rank, i, score, ' '.join(file_tokens_pairs[i]), sep = \"\\t\")\n",
    "        if rank == 10:\n",
    "            break\n",
    "        rank = rank + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
